{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week8_MLP_Quick_WalkThrough.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2SUBDA/Breakouts/blob/Week8/Week8_MLP_Quick_WalkThrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0zqqZwotK_2"
      },
      "source": [
        "# BASIC WALK THROUGH FOR FASHION-MNIST MLP\n",
        "# BASED ON SKLEARN TUTORIALS \n",
        "# ALTERNATIVE TO CREATING MLP MODEL THE OLD FASHIONED WAY\n",
        "\n",
        "from sklearn import datasets, metrics\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhm5xXKytg76"
      },
      "source": [
        "# OBTAIN\n",
        "# THERE ARE MANY SOURCES OF THE MNIST DATA - THIS IS ANOTHER ONE\n",
        "\n",
        "digits = datasets.load_digits()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfbEMtFtr4X"
      },
      "source": [
        "# SCRUB\n",
        "# NORMALIZE INPUTS FROM RGB TO 0-1\n",
        "\n",
        "X = digits.data / 255\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XWy-mvFuQxf"
      },
      "source": [
        "# MODEL\n",
        "# Declare basic model using MLP Classifier\n",
        "# Additional configurations available:  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, alpha=1e-4, solver=\"adam\",\n",
        "                    verbose = 10, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
        "  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHpS6DI-utYP",
        "outputId": "7b992757-acf6-46b1-c021-1f6bd0c6cbd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# MODEL\n",
        "# FIT\n",
        "\n",
        "mlp.fit(X_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.16590220\n",
            "Iteration 2, loss = 1.23173143\n",
            "Iteration 3, loss = 0.58644120\n",
            "Iteration 4, loss = 0.35242122\n",
            "Iteration 5, loss = 0.25859961\n",
            "Iteration 6, loss = 0.21296919\n",
            "Iteration 7, loss = 0.18493035\n",
            "Iteration 8, loss = 0.14931151\n",
            "Iteration 9, loss = 0.12830493\n",
            "Iteration 10, loss = 0.14637506\n",
            "Iteration 11, loss = 0.12458948\n",
            "Iteration 12, loss = 0.10750769\n",
            "Iteration 13, loss = 0.09793221\n",
            "Iteration 14, loss = 0.07278789\n",
            "Iteration 15, loss = 0.07274549\n",
            "Iteration 16, loss = 0.06881495\n",
            "Iteration 17, loss = 0.06414864\n",
            "Iteration 18, loss = 0.04873463\n",
            "Iteration 19, loss = 0.04847816\n",
            "Iteration 20, loss = 0.05129334\n",
            "Iteration 21, loss = 0.05003880\n",
            "Iteration 22, loss = 0.04548845\n",
            "Iteration 23, loss = 0.04128697\n",
            "Iteration 24, loss = 0.03270697\n",
            "Iteration 25, loss = 0.02856366\n",
            "Iteration 26, loss = 0.02644293\n",
            "Iteration 27, loss = 0.02606026\n",
            "Iteration 28, loss = 0.02441724\n",
            "Iteration 29, loss = 0.02181927\n",
            "Iteration 30, loss = 0.02227938\n",
            "Iteration 31, loss = 0.02464276\n",
            "Iteration 32, loss = 0.02003288\n",
            "Iteration 33, loss = 0.02155614\n",
            "Iteration 34, loss = 0.02157935\n",
            "Iteration 35, loss = 0.03033897\n",
            "Iteration 36, loss = 0.02835419\n",
            "Iteration 37, loss = 0.03250416\n",
            "Iteration 38, loss = 0.03115793\n",
            "Iteration 39, loss = 0.02234060\n",
            "Iteration 40, loss = 0.02590582\n",
            "Iteration 41, loss = 0.01761179\n",
            "Iteration 42, loss = 0.01861050\n",
            "Iteration 43, loss = 0.01985384\n",
            "Iteration 44, loss = 0.02025905\n",
            "Iteration 45, loss = 0.01874851\n",
            "Iteration 46, loss = 0.01236542\n",
            "Iteration 47, loss = 0.01332467\n",
            "Iteration 48, loss = 0.01180830\n",
            "Iteration 49, loss = 0.00976098\n",
            "Iteration 50, loss = 0.00856960\n",
            "Iteration 51, loss = 0.00822631\n",
            "Iteration 52, loss = 0.00797750\n",
            "Iteration 53, loss = 0.00869903\n",
            "Iteration 54, loss = 0.00757051\n",
            "Iteration 55, loss = 0.00714714\n",
            "Iteration 56, loss = 0.00684954\n",
            "Iteration 57, loss = 0.00680089\n",
            "Iteration 58, loss = 0.00663765\n",
            "Iteration 59, loss = 0.00678339\n",
            "Iteration 60, loss = 0.00662884\n",
            "Iteration 61, loss = 0.00633782\n",
            "Iteration 62, loss = 0.00637518\n",
            "Iteration 63, loss = 0.00623156\n",
            "Iteration 64, loss = 0.00628208\n",
            "Iteration 65, loss = 0.00675065\n",
            "Iteration 66, loss = 0.00670993\n",
            "Iteration 67, loss = 0.00612747\n",
            "Iteration 68, loss = 0.00634960\n",
            "Iteration 69, loss = 0.00583436\n",
            "Iteration 70, loss = 0.00577390\n",
            "Iteration 71, loss = 0.00571136\n",
            "Iteration 72, loss = 0.00573277\n",
            "Iteration 73, loss = 0.00586208\n",
            "Iteration 74, loss = 0.00550514\n",
            "Iteration 75, loss = 0.00550050\n",
            "Iteration 76, loss = 0.00567975\n",
            "Iteration 77, loss = 0.00537211\n",
            "Iteration 78, loss = 0.00560265\n",
            "Iteration 79, loss = 0.00534226\n",
            "Iteration 80, loss = 0.00524449\n",
            "Iteration 81, loss = 0.00515640\n",
            "Iteration 82, loss = 0.00510060\n",
            "Iteration 83, loss = 0.00501970\n",
            "Iteration 84, loss = 0.00495815\n",
            "Iteration 85, loss = 0.00495713\n",
            "Iteration 86, loss = 0.00493942\n",
            "Iteration 87, loss = 0.00498842\n",
            "Iteration 88, loss = 0.00494938\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(learning_rate_init=0.1, max_iter=100, random_state=1, verbose=10)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFBQCovJu7yP",
        "outputId": "c850c28b-3334-4eff-efaa-daf5cc5cc507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# RECOMMMEND\n",
        "# Performance of model\n",
        "\n",
        "print(\"Training set score: {0}\".format(mlp.score(X_train, y_train)))\n",
        "print()\n",
        "print(\"Test set score: {0}\".format(mlp.score(X_test, y_test)))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 1.0\n",
            "\n",
            "Test set score: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL - WHAT ABOUT CHANGING THE SOLVER?\n",
        "\n",
        "mlpSGD = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, alpha=1e-4, solver=\"sgd\",\n",
        "                    verbose = 10, tol=1e-4, random_state=1, learning_rate_init=.1)\n",
        "  "
      ],
      "metadata": {
        "id": "bBUG0-A3NtIl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL\n",
        "# FIT\n",
        "\n",
        "mlpSGD.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "errkMYdiN5FV",
        "outputId": "01548136-9298-492e-d865-cc45b4bc1dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.30745965\n",
            "Iteration 2, loss = 2.29660002\n",
            "Iteration 3, loss = 2.28960444\n",
            "Iteration 4, loss = 2.28044263\n",
            "Iteration 5, loss = 2.26930470\n",
            "Iteration 6, loss = 2.25739035\n",
            "Iteration 7, loss = 2.24739009\n",
            "Iteration 8, loss = 2.23203051\n",
            "Iteration 9, loss = 2.21891479\n",
            "Iteration 10, loss = 2.20276704\n",
            "Iteration 11, loss = 2.18397042\n",
            "Iteration 12, loss = 2.16436767\n",
            "Iteration 13, loss = 2.14142778\n",
            "Iteration 14, loss = 2.11562117\n",
            "Iteration 15, loss = 2.08854023\n",
            "Iteration 16, loss = 2.05490948\n",
            "Iteration 17, loss = 2.01655071\n",
            "Iteration 18, loss = 1.97599105\n",
            "Iteration 19, loss = 1.93040874\n",
            "Iteration 20, loss = 1.87963067\n",
            "Iteration 21, loss = 1.82556219\n",
            "Iteration 22, loss = 1.76613390\n",
            "Iteration 23, loss = 1.70473499\n",
            "Iteration 24, loss = 1.63840666\n",
            "Iteration 25, loss = 1.57076280\n",
            "Iteration 26, loss = 1.50470217\n",
            "Iteration 27, loss = 1.43665179\n",
            "Iteration 28, loss = 1.37109113\n",
            "Iteration 29, loss = 1.30321126\n",
            "Iteration 30, loss = 1.24247076\n",
            "Iteration 31, loss = 1.18190744\n",
            "Iteration 32, loss = 1.12543010\n",
            "Iteration 33, loss = 1.06904903\n",
            "Iteration 34, loss = 1.01791858\n",
            "Iteration 35, loss = 0.97332422\n",
            "Iteration 36, loss = 0.93089739\n",
            "Iteration 37, loss = 0.88399289\n",
            "Iteration 38, loss = 0.84778959\n",
            "Iteration 39, loss = 0.81035850\n",
            "Iteration 40, loss = 0.77641005\n",
            "Iteration 41, loss = 0.74687765\n",
            "Iteration 42, loss = 0.72122172\n",
            "Iteration 43, loss = 0.69284101\n",
            "Iteration 44, loss = 0.67109792\n",
            "Iteration 45, loss = 0.64624828\n",
            "Iteration 46, loss = 0.62538145\n",
            "Iteration 47, loss = 0.61150748\n",
            "Iteration 48, loss = 0.58689095\n",
            "Iteration 49, loss = 0.56825581\n",
            "Iteration 50, loss = 0.55158402\n",
            "Iteration 51, loss = 0.53889351\n",
            "Iteration 52, loss = 0.52350913\n",
            "Iteration 53, loss = 0.50855356\n",
            "Iteration 54, loss = 0.49601433\n",
            "Iteration 55, loss = 0.48312073\n",
            "Iteration 56, loss = 0.47127481\n",
            "Iteration 57, loss = 0.46316226\n",
            "Iteration 58, loss = 0.44726037\n",
            "Iteration 59, loss = 0.43979130\n",
            "Iteration 60, loss = 0.42852635\n",
            "Iteration 61, loss = 0.41994230\n",
            "Iteration 62, loss = 0.41323218\n",
            "Iteration 63, loss = 0.40261383\n",
            "Iteration 64, loss = 0.39806108\n",
            "Iteration 65, loss = 0.38597323\n",
            "Iteration 66, loss = 0.37801356\n",
            "Iteration 67, loss = 0.37065529\n",
            "Iteration 68, loss = 0.37080224\n",
            "Iteration 69, loss = 0.35817435\n",
            "Iteration 70, loss = 0.35150830\n",
            "Iteration 71, loss = 0.34691363\n",
            "Iteration 72, loss = 0.33937451\n",
            "Iteration 73, loss = 0.33504693\n",
            "Iteration 74, loss = 0.32825825\n",
            "Iteration 75, loss = 0.32251409\n",
            "Iteration 76, loss = 0.31961880\n",
            "Iteration 77, loss = 0.31703166\n",
            "Iteration 78, loss = 0.30835212\n",
            "Iteration 79, loss = 0.30199916\n",
            "Iteration 80, loss = 0.29930901\n",
            "Iteration 81, loss = 0.29541826\n",
            "Iteration 82, loss = 0.29352091\n",
            "Iteration 83, loss = 0.28904440\n",
            "Iteration 84, loss = 0.28572195\n",
            "Iteration 85, loss = 0.27936596\n",
            "Iteration 86, loss = 0.27787581\n",
            "Iteration 87, loss = 0.27129616\n",
            "Iteration 88, loss = 0.26921383\n",
            "Iteration 89, loss = 0.26502852\n",
            "Iteration 90, loss = 0.26445234\n",
            "Iteration 91, loss = 0.25976603\n",
            "Iteration 92, loss = 0.25549764\n",
            "Iteration 93, loss = 0.25112827\n",
            "Iteration 94, loss = 0.25201331\n",
            "Iteration 95, loss = 0.24813850\n",
            "Iteration 96, loss = 0.24610599\n",
            "Iteration 97, loss = 0.24343974\n",
            "Iteration 98, loss = 0.24424466\n",
            "Iteration 99, loss = 0.24000142\n",
            "Iteration 100, loss = 0.23878832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(300,), learning_rate_init=0.1, max_iter=100,\n",
              "              random_state=1, solver='sgd', verbose=10)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RECOMMMEND\n",
        "# Performance of model\n",
        "\n",
        "print(\"Training set score: {0}\".format(mlpSGD.score(X_train, y_train)))\n",
        "print()\n",
        "print(\"Test set score: {0}\".format(mlpSGD.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "id": "7QqXEXYvN7CG",
        "outputId": "607eaab8-43fe-444e-cd1d-4fd698e7b06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.9474940334128878\n",
            "\n",
            "Test set score: 0.9296296296296296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plenty of configuration choices\n",
        "\n",
        "Where to start?  What to consider?  \n",
        "\n",
        "*   Start with SGD and move to ADAM for optimizer \n",
        "*   Start with 10 or 100 for number of iterations\n",
        "*   Start with 30 for hidden layers\n",
        "\n",
        "How does your choice/configuration change accuracy and compute time?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TfZZuzoWOFrt"
      }
    }
  ]
}